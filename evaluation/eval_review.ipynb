{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import statistics\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "from transformers import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Some weights of RobertaModel were not initialized from the model checkpoint\")\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# for relatively import\n",
    "import sys\n",
    "sys.path.append('/nfs/home/tzulinglin/LLM_reviewer/prompt')\n",
    "\n",
    "from prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad13b69f",
   "metadata": {},
   "source": [
    "Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bed0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_name = 'StyleAdv'\n",
    "model_name = \"gpt-4o-mini\"\n",
    "dataset_name = \"PeerRead_iclr_2017\"\n",
    "# dataset_name = \"AgentReview\"\n",
    "\n",
    "data_path = f\"../data/clean_review/API/{dataset_name}/{model_name}/\"\n",
    "gt_root_dir = \"../data/dataset/PeerRead/data/iclr_2017/\"\n",
    "\n",
    "aspect_tag_types=[\"NONE\", \"SUMMARY\", \"MOTIVATION POSITIVE\", \"MOTIVATION NEGATIVE\", \"SUBSTANCE POSITIVE\", \"SUBSTANCE NEGATIVE\", \n",
    "                  \"ORIGINALITY POSITIVE\", \"ORIGINALITY NEGATIVE\", \"SOUNDNESS POSITIVE\", \"SOUNDNESS NEGATIVE\", \n",
    "                  \"CLARITY POSITIVE\", \"CLARITY NEGATIVE\", \"REPLICABILITY POSITIVE\", \"REPLICABILITY NEGATIVE\", \n",
    "                  \"MEANINGFUL COMPARISON POSITIVE\", \"MEANINGFUL COMPARISON NEGATIVE\"]\n",
    "\n",
    "outfile_path = f'result_review/PeerRead_iclr_2017/{model_name}/'\n",
    "outfile_base = '/'.join(outfile_path.split('/')[:2])\n",
    "os.makedirs(outfile_path, exist_ok=True)  \n",
    "\n",
    "\n",
    "print(f\"Attack Name: {attack_name}\\nModel Name: {model_name}\\nDataset Name: {dataset_name}\\nData Path: {data_path}\\nOutput Path: {outfile_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e531f5c",
   "metadata": {},
   "source": [
    "Read LLM-generated Reviews (loop directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73de73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmAspectCoverage = []\n",
    "llm_review = {}\n",
    "\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith('.txt'):\n",
    "        filename = os.path.basename(file).split('.')[0]\n",
    "        with open(os.path.join(data_path, file), 'r') as f:\n",
    "            content = f.read()\n",
    "        content = PromptTemplate.parseManualReview(content)\n",
    "        original_output_review = content[0]\n",
    "        covered = 0\n",
    "        for aspect in aspect_tag_types:\n",
    "            if aspect in original_output_review:\n",
    "                covered += 1\n",
    "        llmAspectCoverage.append(covered / len(aspect_tag_types)) # for Aspect Coverage calculation\n",
    "        llm_review[filename] = original_output_review # for Similarity calculation    \n",
    "            \n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2251117",
   "metadata": {},
   "source": [
    "Read Ground-Truth Reviews written by human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e70ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtAspectCoverage = []\n",
    "gt_review = {}\n",
    "\n",
    "for subfolder in ['train', 'dev', 'test']:\n",
    "    sub_path = os.path.join(gt_root_dir, subfolder, \"reviews_annotated/result\")\n",
    "    for paper_id in sorted(os.listdir(sub_path)):\n",
    "        result_file = os.path.join(sub_path, paper_id, 'result.jsonl')\n",
    "        covered_list = []  \n",
    "        gt_review_paper = []\n",
    "        \n",
    "        covered = 0\n",
    "        with open(result_file, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # for Aspect Coverage calculation\n",
    "                for label in data['labels']:\n",
    "                    start_idx, end_idx, label_name = label\n",
    "                    label_name = label_name.replace(\"_\", \" \").upper()\n",
    "\n",
    "                    if label_name in aspect_tag_types and label_name not in covered_list:\n",
    "                        covered += 1\n",
    "                        covered_list.append(label_name) \n",
    "\n",
    "                # for Similarity calculation\n",
    "                gt_review_paper.append(data['text'])\n",
    "                \n",
    "        gtAspectCoverage.append(covered / len(aspect_tag_types))\n",
    "        gt_review[paper_id] = gt_review_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863597ae",
   "metadata": {},
   "source": [
    "### Aspect Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebda531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aspect Coverage calculation start!\")\n",
    "\n",
    "'''\n",
    "calculate the average aspect coverage of the original (LLM generated) data\n",
    "'''\n",
    "llmAvg = sum(llmAspectCoverage) / len(llmAspectCoverage)\n",
    "llmVar = statistics.variance(llmAspectCoverage)\n",
    "llmMedian = statistics.median(llmAspectCoverage)\n",
    "\n",
    "\n",
    "'''\n",
    "calculate the average aspect coverage of the ground truth (human labeled) data\n",
    "for each paper, count covered aspect set and divide by the total aspect set\n",
    "avg among all papers\n",
    "'''    \n",
    "gtavg = sum(gtAspectCoverage) / len(gtAspectCoverage)\n",
    "gtVar = statistics.variance(gtAspectCoverage)\n",
    "gtMedian = statistics.median(gtAspectCoverage)\n",
    "\n",
    "\n",
    "LLM_data = {\n",
    "        \"type\": \"original\",\n",
    "        'numbers':  len(llmAspectCoverage),\n",
    "        \"avg\": round(llmAvg, 4),\n",
    "        \"var\": round(llmVar, 4),\n",
    "        \"median\": round(llmMedian, 4),\n",
    "    }\n",
    "gt_data = {\n",
    "        \"type\": \"gt\",\n",
    "        'numbers':  len(gtAspectCoverage),\n",
    "        \"avg\": round(gtavg, 4),\n",
    "        \"var\": round(gtVar, 4),\n",
    "        \"median\": round(gtMedian, 4),\n",
    "    }\n",
    "\n",
    "\n",
    "with open(outfile_path+'aspect_coverage.jsonl', 'w') as outfile:\n",
    "    json.dump(LLM_data, outfile)\n",
    "    outfile.write('\\n')\n",
    "    json.dump(gt_data, outfile)\n",
    "    outfile.write('\\n')\n",
    "\n",
    "'''\n",
    "draw box plot of LLMaspectCoverage and GTaspectCoverage\n",
    "'''\n",
    "plt.boxplot([llmAspectCoverage, gtAspectCoverage], labels=[f'{model_name}', 'Human'])\n",
    "plt.ylabel('Aspect Coverage')\n",
    "plt.title(f'Aspect Coverage of {attack_name}')\n",
    "plt.savefig(outfile_path+'aspect_coverage.png')\n",
    "plt.show()\n",
    "    \n",
    "print(\"Aspect Coverage calculation done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac0891",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bb2bb",
   "metadata": {},
   "source": [
    "Compute Similarity Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"], keep_in_memory=True)\n",
    "metric_bertscore = evaluate.load(\"bertscore\", keep_in_memory=True)\n",
    "\n",
    "def compute_similarity_metrics(pred_str, labels_str):\n",
    "    # Compute ROUGE scores\n",
    "    rouge_output = metric_rouge.compute(predictions=pred_str, references=labels_str, use_stemmer=True)\n",
    "    bertscore = metric_bertscore.compute(predictions=pred_str, references=labels_str, lang=\"en\")\n",
    "    return rouge_output[\"rouge1\"], rouge_output[\"rouge2\"], rouge_output[\"rougeL\"], bertscore[\"f1\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9deee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity calculation start!\")\n",
    "\n",
    "calculate_human_pairwise_sim = True\n",
    "if os.path.exists(outfile_base+'similarity_humanpair.jsonl'):\n",
    "    calculate_human_pairwise_sim = False\n",
    "\n",
    "\n",
    "'''\n",
    "calculate the similarity between the original (LLM generated) review and the ground truth (human labeled) review\n",
    "'''\n",
    "print(len(llm_review), len(gt_review))\n",
    "\n",
    "\n",
    "'''\n",
    "calculate the similarity, split by aspect\n",
    "find max among all aspect pairs\n",
    "'''   \n",
    "\n",
    "output = []\n",
    "human_eval = []\n",
    "for paper_id in tqdm(llm_review.keys()):\n",
    "    llm_review_text = \" \".join(llm_review[paper_id].values())\n",
    "    gt_review_text = gt_review[paper_id]\n",
    "    \n",
    "    llm_review_text = [llm_review_text] * len(gt_review_text)\n",
    "    # print(len(llm_review_text), len(gt_review_text))\n",
    "    \n",
    "    max_rouge1, max_rouge2, max_rougeL, max_bertscore = 0, 0, 0, 0\n",
    "    max_total = 0\n",
    "    chosen_review_id = -1\n",
    "    for review_id, (llm_str, gt_str) in enumerate(zip(llm_review_text, gt_review_text)):\n",
    "        rouge1, rouge2, rougeL, bertscore = compute_similarity_metrics([llm_str], [gt_str])\n",
    "        total = rouge1 + rouge2 + rougeL + bertscore\n",
    "        \n",
    "        # choose the review with the highest total score\n",
    "        if  total > max_total:\n",
    "            max_total = total\n",
    "            max_rouge1, max_rouge2, max_rougeL, max_bertscore = rouge1, rouge2, rougeL, bertscore  \n",
    "            chosen_review_id = review_id\n",
    "        \n",
    "        output.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"chosen_review_id\": chosen_review_id,\n",
    "            \"rouge1\": round(max_rouge1, 4),\n",
    "            \"rouge2\": round(max_rouge2, 4),\n",
    "            \"rougeL\": round(max_rougeL, 4),\n",
    "            \"bertscore\": round(max_bertscore, 4),\n",
    "        })\n",
    "           \n",
    "           \n",
    "        # human pairwise evaluation within paper\n",
    "        if calculate_human_pairwise_sim:\n",
    "            gt_review_combination = list(itertools.combinations(gt_review_text, 2))\n",
    "            max_rouge1_humanpair, max_rouge2_humanpair, max_rougeL_humanpair, max_bertscore_humanpair = 0, 0, 0, 0\n",
    "            max_total_humanpair = 0\n",
    "            chosen_review_id = -1\n",
    "            for gt_review1, gt_review2 in gt_review_combination:\n",
    "                rouge1_humanpair, rouge2_humanpair, rougeL_humanpair, bertscore_humanpair = compute_similarity_metrics([gt_review1], [gt_review2])\n",
    "                # print(f\"rouge1: {rouge1}, rouge2: {rouge2}, rougeL: {rougeL}, bertscore: {bertscore}\")\n",
    "\n",
    "                total_humanpair = rouge1_humanpair + rouge2_humanpair + rougeL_humanpair + bertscore_humanpair\n",
    "                \n",
    "                # choose the review with the highest total score\n",
    "                if  total_humanpair > max_total_humanpair:\n",
    "                    max_total_humanpair = total_humanpair\n",
    "                    max_rouge1_humanpair, max_rouge2_humanpair, max_rougeL_humanpair, max_bertscore_humanpair = rouge1_humanpair, rouge2_humanpair, rougeL_humanpair, bertscore_humanpair  \n",
    "                    chosen_review_id = review_id\n",
    "                    \n",
    "            human_eval.append({\n",
    "                \"paper_id\": paper_id,\n",
    "                \"rouge1\": round(max_rouge1_humanpair, 4),\n",
    "                \"rouge2\": round(max_rouge2_humanpair, 4),\n",
    "                \"rougeL\": round(max_rougeL_humanpair, 4),\n",
    "                \"bertscore\": round(max_bertscore_humanpair, 4),\n",
    "            })\n",
    "                \n",
    "\n",
    "# avg among all papers\n",
    "avg_rouge1 = round(sum([line['rouge1'] for line in output]) / len(output), 4)\n",
    "avg_rouge2 = round(sum([line['rouge2'] for line in output]) / len(output), 4)\n",
    "avg_rougeL = round(sum([line['rougeL'] for line in output]) / len(output), 4)\n",
    "avg_bertscore = round(sum([line['bertscore'] for line in output]) / len(output), 4)\n",
    "print(f\"[similarity pred/gt] avg rouge1: {avg_rouge1}, avg rouge2: {avg_rouge2}, avg rougeL: {avg_rougeL}, avg bertscore: {avg_bertscore}\")\n",
    "\n",
    "with open(outfile_path+'similarity.jsonl', 'w') as outfile:\n",
    "    outfile.write(f\"avg rouge1: {avg_rouge1}, avg rouge2: {avg_rouge2}, avg rougeL: {avg_rougeL}, avg bertscore: {avg_bertscore} \\n\")\n",
    "    for line in output:\n",
    "        json.dump(line, outfile)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "if calculate_human_pairwise_sim:\n",
    "    # avg among all papers\n",
    "    avg_rouge1_humanpair = round(sum([line['rouge1'] for line in human_eval]) / len(human_eval), 4)\n",
    "    avg_rouge2_humanpair = round(sum([line['rouge2'] for line in human_eval]) / len(human_eval), 4)\n",
    "    avg_rougeL_humanpair = round(sum([line['rougeL'] for line in human_eval]) / len(human_eval), 4)\n",
    "    avg_bertscore_humanpair = round(sum([line['bertscore'] for line in human_eval]) / len(human_eval), 4)\n",
    "    print(f\"[similarity human] avg rouge1: {avg_rouge1_humanpair}, avg rouge2: {avg_rouge2_humanpair}, avg rougeL: {avg_rougeL_humanpair}, avg bertscore: {avg_bertscore_humanpair}\")\n",
    "        \n",
    "    with open(outfile_base+'similarity_humanpair.jsonl', 'w') as outfile:\n",
    "        outfile.write(f\"avg rouge1: {avg_rouge1_humanpair}, avg rouge2: {avg_rouge2_humanpair}, avg rougeL: {avg_rougeL_humanpair}, avg bertscore: {avg_bertscore_humanpair} \\n\")\n",
    "        for line in human_eval:\n",
    "            json.dump(line, outfile)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "print(\"Similarity calculation done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_reviewer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
